# Scraping de Artigos CientÃ­ficos - PROLAM/USP

Este projeto realiza o scraping de artigos cientÃ­ficos da revista PROLAM da USP, organizando-os por data de publicaÃ§Ã£o. A aplicaÃ§Ã£o segue os princÃ­pios de Domain-Driven Design (DDD) e Arquitetura Hexagonal para garantir uma estrutura limpa, testÃ¡vel e de fÃ¡cil manutenÃ§Ã£o.

## ğŸ“‚ Estrutura do Projeto

```bash
scrapping_BJLAS/
â”œâ”€â”€ core/                    # LÃ³gica central de negÃ³cio
â”‚   â”œâ”€â”€ domain/             # Modelos de domÃ­nio
â”‚   â”‚   â”œâ”€â”€ entities.py         # Entidades (Artigo, EdiÃ§Ã£o)
â”‚   â”‚   â”œâ”€â”€ value_objects.py    # Objetos de valor (Slug, DataFormatada)
â”‚   â”‚   â””â”€â”€ repositories.py     # Interfaces de repositÃ³rio
â”‚   â”œâ”€â”€ application/        # Casos de uso
â”‚   â”‚   â””â”€â”€ scraping_service.py  # ServiÃ§o de scraping
â”‚   â””â”€â”€ ports/              # Portas de entrada/saÃ­da
â”‚       â””â”€â”€ logger_port.py       # Interface de logging
â”œâ”€â”€ infrastructure/         # ImplementaÃ§Ãµes concretas
â”‚   â”œâ”€â”€ adapters/           # Adaptadores para serviÃ§os externos
â”‚   â”‚   â”œâ”€â”€ http_edicao_repository.py   # Adaptador HTTP
â”‚   â”‚   â”œâ”€â”€ file_artigo_repository.py   # Adaptador de arquivos
â”‚   â”‚   â””â”€â”€ tqdm_logger.py            # Adaptador de logging
â”‚   â””â”€â”€ config.py           # ConfiguraÃ§Ãµes globais
â”œâ”€â”€ interfaces/             # Pontos de entrada da aplicaÃ§Ã£o
â”‚   â””â”€â”€ cli/
â”‚       â””â”€â”€ main.py             # Script principal
â”œâ”€â”€ tests/                  # Testes unitÃ¡rios automatizados
â”‚   â””â”€â”€ test_scraping_service.py
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â”œâ”€â”€ LICENSE                 # LicenÃ§a MIT
â””â”€â”€ README.md               # DocumentaÃ§Ã£o
```

## ğŸ”§ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### Requisitos

- Python 3.10 ou superior

### Passos

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/projeto_scraping.git
cd scrapping_BJLAS

# Instale as dependÃªncias
pip install -r requirements.txt

# Execute o script principal
python interfaces/cli/main.py
```

## âš™ï¸ ConfiguraÃ§Ãµes

Arquivo: `infrastructure/config.py`

```python
DOWNLOAD_DIR = "pdfs_prolam"
MAX_THREADS = 50
TIMEOUT = 15
URLS = [
    "https://revistas.usp.br/prolam/issue/archive",
    "https://revistas.usp.br/prolam/issue/archive/2"
]
```

## ğŸ§  Arquitetura Hexagonal
![Diagrama mostrando a CLI conectada ao ScrapingService, que usa repositÃ³rios e logger](docs/images/arquitetura_hexagonal.png)

## ğŸ§  PrincÃ­pios de Design

- **DDD (Domain-Driven Design)**

  - Entidades: `Artigo`, `Edicao`
  - Objetos de Valor: `Slug`, `DataFormatada`
  - RepositÃ³rios: `ArtigoRepository`, `EdicaoRepository`

- **InversÃ£o de DependÃªncia**

  - DomÃ­nio independe de infraestrutura
  - AplicaÃ§Ã£o injeta portas (repositÃ³rios e loggers)

- **Testabilidade**

  - Tudo que depende de IO estÃ¡ adaptado e pode ser mockado facilmente

## ğŸš€ Fluxo de ExecuÃ§Ã£o
![Fluxograma do processo de scraping desde a CLI atÃ© o relatÃ³rio final](docs/images/fluxo_execucao.png)

## ğŸ“„ Exemplo de SaÃ­da

```text
[14:30:25] âš¡ Coletando ediÃ§Ãµes em modo turbo...
[14:30:26] âœ… 50 ediÃ§Ãµes detectadas
[14:30:26] âš¡ Extraindo artigos em paralelo...
Processando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:05<00:00, 10.00ediÃ§Ã£o/s]
[14:30:31] ğŸš€ 523 artigos preparados para download
[14:30:31] âš¡ Iniciando downloads paralelos...
Baixando: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 523/523 [00:35<00:00, 14.90PDF/s]

============================================================
ğŸ DESEMPENHO FINAL
â€¢ EdiÃ§Ãµes processadas: 50
â€¢ Artigos encontrados: 523
â€¢ Downloads concluÃ­dos: 523
â€¢ Tempo total: 40.21 segundos
â€¢ Velocidade: 13.0 artigos/segundo
ğŸ”¥ Taxa de transferÃªncia estimada: 32.5 MB/s
============================================================
```

## ğŸ§ª Testes Automatizados

```bash
# Executar todos os testes
pytest
```

### Arquivo de exemplo

- `tests/test_scraping_service.py`: cobre os fluxos de sucesso, falha por tipo de conteÃºdo e exceÃ§Ã£o em ediÃ§Ã£o.

## ğŸŒŸ BenefÃ­cios da Arquitetura

- **Manutenibilidade**: camadas desacopladas
- **Testabilidade**: camadas centrais independentes de framework
- **Escalabilidade**: fÃ¡cil adicionar banco, cache, UI web
- **Clareza**: separaÃ§Ã£o de responsabilidades clara

## ğŸš¨ Melhorias Futuras

- Interface web com FastAPI
- Cache de ediÃ§Ãµes e artigos
- Dashboard com estatÃ­sticas
- Banco de dados para metadados
- Monitoramento com Prometheus
- Upload automatizado para S3 ou GDrive

